{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports necessary to execute the code\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pooch\n",
    "import tifffile\n",
    "import torch\n",
    "from careamics import CAREamist\n",
    "from careamics.config import GaussianMixtureNMConfig, create_microsplit_configuration, create_n2v_configuration\n",
    "from careamics.lightning import (\n",
    "    create_microsplit_predict_datamodule,\n",
    "    create_microsplit_train_datamodule,\n",
    ")\n",
    "from careamics.lightning.callbacks import DataStatsCallback\n",
    "from careamics.lightning.lightning_module import VAEModule\n",
    "from careamics.lvae_training.dataset import DataSplitType\n",
    "from careamics.lvae_training.eval_utils import get_device\n",
    "from careamics.models.lvae.noise_models import (\n",
    "    GaussianMixtureNoiseModel,\n",
    "    create_histogram,\n",
    ")\n",
    "from careamics.prediction_utils import convert_outputs_microsplit\n",
    "from careamics.utils.metrics import psnr\n",
    "from PIL import Image\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from utils import get_train_val_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the dataset\n",
    "\n",
    "The dataset can be directly downloaded using the `careamics-portfolio` package, which\n",
    "uses `pooch` to download the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_output"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO replace with PortfolioManager\n",
    "DATA = pooch.create(\n",
    "    path=\"./data/\",\n",
    "    base_url=\"https://download.fht.org/jug/msplit/ht_lif24/data_tiff/\",\n",
    "    registry={\"ht_lif24_5ms_reduced.zip\": None},\n",
    ")\n",
    "for fname in DATA.registry:\n",
    "    DATA.fetch(fname, processor=pooch.Unzip(), progressbar=True)\n",
    "\n",
    "DATA_PATH = DATA.abspath / (DATA.registry_files[0] + \".unzip/5ms/data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nm_input = input_data = get_train_val_data(\n",
    "    datadir=DATA_PATH,\n",
    "    datasplit_type=DataSplitType.Train,\n",
    "    val_fraction=0.1,\n",
    "    test_fraction=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NM_PATH = Path(\"./noise_models/\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax[0].imshow(nm_input[0, ..., 0])\n",
    "ax[0].set_title(\"Input channel 1\")\n",
    "ax[1].imshow(nm_input[0, ..., 1])\n",
    "ax[1].set_title(\"Input channel 2\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with the CAREamics Lightning API\n",
    "\n",
    "Using the Lightning API of CAREamics, you need to instantiate the lightning module, the \n",
    "data module and the trainer yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Noise Models\n",
    "\n",
    "Please note that for this step we'll use the high-level CAREamics API. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure N2V\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = create_n2v_configuration(\n",
    "    experiment_name=\"my_data_noise_models_n2v\",\n",
    "    data_type=\"array\",\n",
    "    axes=\"SYXC\",\n",
    "    n_channels=2,\n",
    "    patch_size=(64, 64),\n",
    "    batch_size=64,\n",
    "    num_epochs=5,  # We set the training to 5 epochs, but you can change this to a higher number if you want a better Noise Model.\n",
    ")\n",
    "\n",
    "print(\"N2V configuration generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train N2V on the data we prepared\n",
    "This might take a while, mainly if you changed `num_epochs` above or if you do not have a quick GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "careamist = CAREamist(source=config, work_dir=\"noise_models\")\n",
    "careamist.train(train_source=nm_input, val_minimum_split=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Denoise loaded data with the N2V model we just trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = careamist.predict(nm_input, tile_size=(256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make your choice. If 'False', the entire image will be shown...\n",
    "do_crop = True\n",
    "\n",
    "xfrom = yfrom = 0\n",
    "xto = yto = -1\n",
    "strcrop = \"\"\n",
    "if do_crop:\n",
    "    strcrop = \" (crop)\"\n",
    "    yfrom = 200\n",
    "    yto = 600\n",
    "    xfrom = 800\n",
    "    xto = 1200\n",
    "\n",
    "_, ax = plt.subplots(2, 2, figsize=(10, 10))\n",
    "ax[0][0].imshow(nm_input[0, ..., 0][yfrom:yto, xfrom:xto])\n",
    "ax[0][0].set_title(\"Input channel 1\" + strcrop)\n",
    "ax[0][1].imshow(prediction[0].squeeze()[0][yfrom:yto, xfrom:xto])\n",
    "ax[0][1].set_title(\"Denoised channel 1\" + strcrop)\n",
    "ax[1][0].imshow(nm_input[0, ..., 1][yfrom:yto, xfrom:xto])\n",
    "ax[1][0].set_title(\"Input channel 2\" + strcrop)\n",
    "ax[1][1].imshow(prediction[0].squeeze()[1][yfrom:yto, xfrom:xto])\n",
    "ax[1][1].set_title(\"Denoised channel 2\" + strcrop)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Noise Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for channel_idx in range(nm_input.shape[-1]):\n",
    "\n",
    "    # train Noise Model for current channel\n",
    "    print(f\"Training noise model for channel {channel_idx}\")\n",
    "    channel_data = nm_input[..., channel_idx]\n",
    "    channel_prediction = np.concatenate(prediction)[:, channel_idx]\n",
    "    noise_model_config = GaussianMixtureNMConfig(\n",
    "        model_type=\"GaussianMixtureNoiseModel\",\n",
    "        min_signal=channel_data.min(),\n",
    "        max_signal=channel_data.max(),\n",
    "        n_coeff=3,\n",
    "        n_gaussian=3,\n",
    "    )\n",
    "    noise_model = GaussianMixtureNoiseModel(noise_model_config)\n",
    "    noise_model.fit(signal=channel_data, observation=channel_prediction, n_epochs=1000)\n",
    "\n",
    "    # save result on disk for later re-use\n",
    "    noise_model.save(path=\"noise_models/\", name=f\"noise_model_Ch{channel_idx}\")\n",
    "\n",
    "    # show the result\n",
    "    histogram = create_histogram(\n",
    "        bins=100,\n",
    "        min_val=channel_data.min(),\n",
    "        max_val=channel_data.max(),\n",
    "        signal=channel_data,\n",
    "        observation=channel_prediction,\n",
    "    )\n",
    "\n",
    "# TODO NM preparation needs refactoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Lightning module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = create_microsplit_configuration(\n",
    "    experiment_name=\"ht_lif24_5ms_reduced\",\n",
    "    data_type=\"tiff\", # TODO: after refactoring should mean data extension. Originally reffered to Dataset( e.g. ht_lif24)\n",
    "    axes=\"SYX\",\n",
    "    z_dims=[32] * 4,\n",
    "    patch_size=(64, 64),\n",
    "    grid_size=32,\n",
    "    output_channels=2,\n",
    "    multiscale_count=3,\n",
    "    batch_size=64,\n",
    "    num_epochs=10,\n",
    "    predict_logvar=\"pixelwise\",\n",
    "    nm_paths=[\"/home/igor.zubarev/projects/microSplit-reproducibility/examples/2D/custom_dataset/noise_models/noise_model_Ch0.npz\", \"/home/igor.zubarev/projects/microSplit-reproducibility/examples/2D/custom_dataset/noise_models/noise_model_Ch1.npz\"],\n",
    "    # TODO path to be changed after NM refactoring\n",
    "    train_dataloader_params={\"num_workers\": 0},\n",
    "    val_dataloader_params={\"num_workers\": 0},\n",
    "    logger=None,\n",
    ")\n",
    "\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the data module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAEModule(config.algorithm_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional: Load model from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: move to somewhere\n",
    "def load_pretrained_model(model: VAEModule, ckpt_path):\n",
    "    device = get_device()\n",
    "    ckpt_dict = torch.load(ckpt_path, map_location=device, weights_only=True)\n",
    "    model.load_state_dict(ckpt_dict['state_dict'], strict=False)\n",
    "    print(f\"Loaded model from {ckpt_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_pretrained_model(model, ckpt_path) # TODO optional, temporary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_module = create_microsplit_train_datamodule(\n",
    "    train_data=DATA_PATH,\n",
    "    data_type=config.data_config.data_type,\n",
    "    patch_size=config.data_config.image_size, # TODO, it's not patch size because of ugly duplication\n",
    "    grid_size=config.data_config.grid_size,\n",
    "    multiscale_count=config.data_config.multiscale_lowres_count,# TODO, same, because of ugly duplication\n",
    "    axes=config.data_config.axes,\n",
    "    batch_size=config.data_config.batch_size, # TODO, should be inside dataloader params?\n",
    "    transforms=[],\n",
    "    train_dataloader_params=config.data_config.train_dataloader_params,\n",
    "    val_dataloader_params=config.data_config.val_dataloader_params,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the trainer\n",
    "\n",
    "Note that here we modify the prediction loop, but this will be  changed in the near\n",
    "future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_output"
    ]
   },
   "outputs": [],
   "source": [
    "# Create Callbacks\n",
    "root = Path(\"ht_lif24\")\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        dirpath=root / \"checkpoints\",\n",
    "        filename=\"ht_lif24_lightning_api\",\n",
    "        save_last=True,\n",
    "    ),\n",
    "    DataStatsCallback()\n",
    "]\n",
    "\n",
    "# Create a Lightning Trainer\n",
    "trainer = Trainer(\n",
    "    max_epochs=config.training_config.lightning_trainer_config[\"max_epochs\"],\n",
    "    default_root_dir=root,\n",
    "    callbacks=callbacks,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer.fit(model, datamodule=train_data_module)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict with CAREamics Lightning API\n",
    "\n",
    "### Define the prediction datamodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO ugly that we need to initiliaze train_data_module to get data stats\n",
    "data_stats, max_val = train_data_module.get_data_stats()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_data_module = create_microsplit_predict_datamodule(\n",
    "    pred_data=DATA_PATH,\n",
    "    data_type=\"tiff\", # TODO see train dm\n",
    "    axes=\"YX\",\n",
    "    batch_size=64,\n",
    "    multiscale_count=config.data_config.multiscale_lowres_count,\n",
    "    data_stats=data_stats,\n",
    "    max_val=max_val, # TODO should be in the config?\n",
    "    tile_size=(64, 64),\n",
    "    grid_size=32, # TODO rename to overlap\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_output"
    ]
   },
   "outputs": [],
   "source": [
    "# Predict\n",
    "predicted_tiles = trainer.predict(model, datamodule=pred_data_module)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_tiles[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(predicted_tiles[0][0][1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the outputs to the original format, mostly useful if tiling is used\n",
    "predictions, _ = convert_outputs_microsplit(predicted_tiles, pred_data_module.predict_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(predictions[10, 200:700, 200:700, 1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show two images\n",
    "noises = [tifffile.imread(f) for f in sorted(test_path.glob(\"*.tiff\"))]\n",
    "gts = [tifffile.imread(f) for f in sorted(gt_path.glob(\"*.tiff\"))]\n",
    "\n",
    "# images to show\n",
    "images = np.random.choice(range(len(noises)), 3)\n",
    "\n",
    "fig, ax = plt.subplots(3, 3, figsize=(15, 15))\n",
    "fig.tight_layout()\n",
    "\n",
    "for i in range(3):\n",
    "    pred_image = prediction[images[i]].squeeze()\n",
    "    psnr_noisy = psnr(\n",
    "        gts[images[i]], noises[images[i]], data_range=noises[images[i]].max()\n",
    "    )\n",
    "    psnr_result = psnr(gts[images[i]], pred_image, pred_image.max())\n",
    "\n",
    "    ax[i, 0].imshow(noises[images[i]], cmap=\"gray\")\n",
    "    ax[i, 0].title.set_text(f\"Noisy\\nPSNR: {psnr_noisy:.2f}\")\n",
    "\n",
    "    ax[i, 1].imshow(pred_image, cmap=\"gray\")\n",
    "    ax[i, 1].title.set_text(f\"Prediction\\nPSNR: {psnr_result:.2f}\")\n",
    "\n",
    "    ax[i, 2].imshow(gts[images[i]], cmap=\"gray\")\n",
    "    ax[i, 2].title.set_text(\"Ground-truth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psnrs = np.zeros((len(prediction), 1))\n",
    "\n",
    "for i, (pred, gt) in enumerate(zip(prediction, gts)):\n",
    "    psnrs[i] = psnr(gt, pred.squeeze(), pred.max())\n",
    "\n",
    "print(f\"PSNR: {psnrs.mean():.2f} +/- {psnrs.std():.2f}\")\n",
    "print(\"Reported PSNR: 27.71\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create cover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a cover image\n",
    "im_idx = 3\n",
    "cv_image_noisy = noises[im_idx]\n",
    "cv_image_pred = prediction[im_idx].squeeze()\n",
    "\n",
    "# create image\n",
    "cover = np.zeros((256, 256))\n",
    "(height, width) = cv_image_noisy.shape\n",
    "assert height > 256\n",
    "assert width > 256\n",
    "\n",
    "# normalize train and prediction\n",
    "norm_noise = (cv_image_noisy - cv_image_noisy.min()) / (\n",
    "    cv_image_noisy.max() - cv_image_noisy.min()\n",
    ")\n",
    "norm_pred = (cv_image_pred - cv_image_pred.min()) / (\n",
    "    cv_image_pred.max() - cv_image_pred.min()\n",
    ")\n",
    "\n",
    "# fill in halves\n",
    "cover[:, : 256 // 2] = norm_noise[\n",
    "    height // 2 - 256 // 2 : height // 2 + 256 // 2, width // 2 - 256 // 2 : width // 2\n",
    "]\n",
    "cover[:, 256 // 2 :] = norm_pred[\n",
    "    height // 2 - 256 // 2 : height // 2 + 256 // 2, width // 2 : width // 2 + 256 // 2\n",
    "]\n",
    "\n",
    "# plot the single image\n",
    "plt.imshow(cover, cmap=\"gray\")\n",
    "\n",
    "# save the image\n",
    "im = Image.fromarray(cover * 255)\n",
    "im = im.convert(\"L\")\n",
    "im.save(\"BSD68_Noise2Void_lightning_api.jpeg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
